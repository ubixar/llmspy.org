---
title: Quick Start
description: Get started with llms.py in minutes
---

## 1. Set API Keys

Set the API keys for the providers you want to use:

<ShellCommand>export OPENROUTER_API_KEY = "..."</ShellCommand>

API Keys used for the default providers in [llms.json](https://github.com/ServiceStack/llms/blob/main/llms/llms.json):

| Provider | Environment Variable | Description |
|----------|---------------------|-------------|
| openrouter_free | `OPENROUTER_API_KEY` | OpenRouter FREE models |
| groq | `GROQ_API_KEY` | Groq API key |
| google_free | `GOOGLE_FREE_API_KEY` | Google FREE API key |
| codestral | `CODESTRAL_API_KEY` | Codestral API key |
| ollama | N/A | No API key required |
| openrouter | `OPENROUTER_API_KEY` | OpenRouter API key |
| google | `GOOGLE_API_KEY` | Google API key |
| anthropic | `ANTHROPIC_API_KEY` | Anthropic API key |
| openai | `OPENAI_API_KEY` | OpenAI API key |
| grok | `GROK_API_KEY` | Grok (X.AI) API key |
| qwen | `DASHSCOPE_API_KEY` | Qwen (Alibaba) key |
| z.ai | `ZAI_API_KEY` | Z.ai API key |
| mistral | `MISTRAL_API_KEY` | Mistral API key |

## 2. Start the Server

Launch the web UI and API server:

<ShellCommand>llms --serve 8000</ShellCommand>

This starts:
- Web UI at `http://localhost:8000`
- OpenAI-compatible API at `http://localhost:8000/v1/chat/completions`

#### For detailed logging:

<ShellCommand>llms --serve 8000 --verbose</ShellCommand>

## Enable Providers

You can enable/disable providers at runtime in the UI or CLI:

<ShellCommand>llms --enable openrouter_free google_free groq</ShellCommand>

<ShellCommand>llms --disable openai anthropic grok</ShellCommand>



## Using the Web UI

Once the server is running, open `http://localhost:8000` in your browser.

### Features

- **Chat Interface**: ChatGPT-like interface for conversations
- **Model Selection**: Choose from all enabled providers and models
- **System Prompts**: Access 200+ professional system prompts
- **File Attachments**: Upload images, audio, and documents
- **Dark Mode**: Toggle between light and dark themes
- **Analytics**: Track costs, tokens, and usage
- **Search**: Find past conversations easily

### Keyboard Shortcuts

- `Enter` - Send message (or `Ctrl/Cmd + Enter` for new line)
- `/` - Focus search
- `Esc` - Close dialogs

## OpenAI-Compatible API

Use the server as an OpenAI-compatible endpoint:

```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "grok-4-fast",
    "messages": [
      {"role": "user", "content": "Hello!"}
    ]
  }'
```

This works with any OpenAI-compatible client library.

## Provider Management

### List Providers

```bash
# List all providers
llms --list
llms ls

# List specific providers
llms ls groq anthropic
```

### Enable/Disable Providers

```bash
# Disable providers
llms --disable openrouter_free codestral

# Enable providers
llms --enable openai grok
```

Providers are invoked in the order they're defined in `llms.json`. If one fails, it automatically tries the next available provider.

## Check Provider Status

Test provider connectivity and response times:

```bash
# Check all models for a provider
llms --check groq

# Check specific models
llms --check groq kimi-k2 llama4:400b
```

This helps verify providers are configured correctly and shows their response times.

## Next Steps

<Cards>
  <Card title="Web UI Features" href="/docs/features/web-ui" />
  <Card title="Configuration" href="/docs/configuration" />
  <Card title="CLI Reference" href="/docs/cli" />
  <Card title="Multimodal Support" href="/docs/features/multimodal" />
</Cards>
