---
title: Using llama.cpp with llms-py
description: Run local LLMs using llama.cpp integration with llms-py.
---

The [llama-cpp](https://github.com/ServiceStack/llms/tree/main/llms/extensions/llama_cpp) extension enables running local LLMs using [llama.cpp](https://github.com/ggerganov/llama.cpp), providing a high-performance inference engine that runs entirely on your local machine.

### Why llama.cpp?

- **Privacy First**: All inference happens locally — no data leaves your machine
- **Cost Free**: No API bills — use any GGUF model you already have
- **Hardware Acceleration**: Optimized for CPU (and GPU via cuBLAS, Metal, etc.)
- **Model Flexibility**: Support for any model in GGUF format

### Prerequisites

#### Install llama.cpp

<Tabs>
<Tab title="macOS (Homebrew)">

<ShellCommand>brew install llama-cpp-llama</ShellCommand>

</Tab>
<Tab title="Linux">

```bash
# Build from source
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make
```

Or download pre-built binaries from the [releases page](https://github.com/ggerganov/llama.cpp/releases).

</Tab>
<Tab title="Windows">

Download pre-built binaries from the [releases page](https://github.com/ggerganov/llama.cpp/releases).

</Tab>
</Tabs>

#### Download a GGUF Model

Popular GGUF models available for download:

- [Llama 3.2](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct-GGUF)
- [Qwen 2.5](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-GGUF)
- [Mistral 7B](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF)
- [Phi-3.5](https://huggingface.co/microsoft/Phi-3.5-mini-instruct-gguf)

### Configuration

#### Start the llama.cpp Server

<ShellCommand>llama-server -m your-model.gguf -c 4096 --host 127.0.0.1 --port 8080</ShellCommand>

Common options:
- `-c`: Context window size (default: 512)
- `--host`: Bind address (default: 127.0.0.1)
- `--port`: Port to listen on (default: 8080)
- `--threads`: Number of CPU threads to use

#### Connect llms-py to llama.cpp

```json
{
  "llama.cpp": {
    "enabled": true,
    "baseUrl": "http://127.0.0.1:8080",
    "defaultModel": "your-model.gguf"
  }
}
```

### Usage Examples

#### Basic Chat

```python
from llms import Llm

llm = Llm(provider="llama.cpp")
response = llm.chat("Explain quantum computing in simple terms")
print(response)
```

#### Streaming Responses

```python
llm = Llm(provider="llama.cpp")

async for chunk in llm.chat("Write a story about a robot", stream=True):
    print(chunk, end="", flush=True)
```

#### Using Different Models

```python
# Use a specific model
llm = Llm(provider="llama.cpp", model="llama-3.2-3b-instruct.gguf")

# List available models
models = llm.models()
print(models)
```

### API Reference

#### LlmOptions

| Option | Type | Description |
|--------|------|-------------|
| `model` | `str` | The GGUF model filename to use |
| `temperature` | `float` | Sampling temperature (default: 0.7) |
| `maxTokens` | `int` | Maximum tokens to generate |
| `stream` | `bool` | Enable streaming responses |

#### Environment Variables

| Variable | Description |
|----------|-------------|
| `LLAMA_CPP_URL` | Base URL for llama.cpp server (default: http://127.0.0.1:8080) |
| `LLAMA_CPP_MODEL` | Default model to use |

### Troubleshooting

#### Connection Refused

Ensure the llama.cpp server is running and accessible at the configured URL:

<ShellCommand># Check if server is running
curl http://127.0.0.1:8080/health</ShellCommand>

#### Out of Memory (OOM)

- Reduce context window size with `-c`
- Use a smaller model
- Reduce batch size in llama.cpp flags

#### Slow Inference

- Increase `--threads` for more CPU cores
- Use GPU acceleration if available (cuBLAS, Metal, etc.)
- Consider using a quantized model (Q4_K_M, Q5_K_M for balance of quality/speed)

### Next Steps

- [Model Selector UI](/docs/features/model-selector) — Browse and switch between local and remote models
- [Extensions](/docs/extensions) — Extend functionality with additional features
- [CLI Reference](/docs/features/cli) — Command-line interface for local inference
