---
title: CLI
description: CLI interface for all your LLMs
---

## Using the CLI

Ask questions directly from the command line:

```bash
# Simple query
llms "What is the capital of France?"

# With specific model
llms -m grok-4-fast "Explain quantum computing"

# With system prompt
llms -s "You are a helpful coding assistant" "How do I reverse a string in Python?"

# With image
llms --image photo.jpg "What's in this image?"

# With audio
llms --audio recording.mp3 "Transcribe this audio"

# With file
llms --file document.pdf "Summarize this PDF"
```

## 5. Configure Default Model

Set your preferred default model:

<ShellCommand>llms --default grok-4-fast</ShellCommand>

## Common CLI Examples

### Text Generation

```bash
# Basic chat
llms "Explain quantum computing in simple terms"

# With specific model
llms -m gemini-2.5-pro "Write a Python function to sort a list"

# With system prompt
llms -s "You are a quantum expert" "Explain quantum computing"

# Display full JSON response
llms "Hello" --raw
```

### Image Analysis

```bash
# Local image
llms --image screenshot.png "What's in this image?"

# Remote image
llms --image https://example.com/photo.jpg "Describe this photo"

# With specific vision model
llms -m gemini-2.5-flash --image chart.png "Analyze this chart"
```

### Audio Transcription

```bash
# Transcribe audio
llms --audio meeting.wav "Summarize this meeting recording"

# With specific audio model
llms -m gpt-4o-audio-preview --audio interview.mp3 "Extract main topics"
```

### Document Processing

```bash
# Summarize PDF
llms --file report.pdf "Summarize the key points"

# Extract data
llms -m gemini-flash-latest --file policy.pdf "Extract action items"
```


## CLI Reference

### Basic Usage

```bash
llms [OPTIONS] [PROMPT]
```

### Chat

```bash
# Simple query
llms "What is the capital of France?"

# With specific model
llms -m grok-4-fast "Explain quantum computing"

# With system prompt
llms -s "You are a helpful assistant" "Write a Python function"
```

### Server

```bash
# Start server on port 8000
llms --serve 8000

# With verbose logging
llms --serve 8000 --verbose
```

### Configuration

```bash
# Initialize configuration
llms --init

# List providers and models
llms --list
llms ls

# Enable/disable providers
llms --enable groq openai
llms --disable ollama

# Set default model
llms --default grok-4-fast

# Check provider status
llms --check groq
```

## Options Reference

### Model & Provider Options

#### `-m, --model MODEL`

Specify which model to use:

```bash
llms -m grok-4-fast "Hello"
llms -m gemini-2.5-pro "Explain quantum physics"
```

#### `-s, --system PROMPT`

Set system prompt:

```bash
llms -s "You are a helpful coding assistant" "How do I sort an array?"
```

### Input Options

#### `--image IMAGE`

Process image input:

```bash
llms --image photo.jpg "What's in this image?"
llms --image https://example.com/chart.png "Analyze this chart"
```

#### `--audio AUDIO`

Process audio input:

```bash
llms --audio recording.mp3 "Transcribe this"
llms --audio meeting.wav "Summarize this meeting"
```

#### `--file FILE`

Process file/document input:

```bash
llms --file document.pdf "Summarize this PDF"
llms --file report.pdf "Extract key points"
```

#### `--chat REQUEST`

Use custom chat completion request:

```bash
llms --chat request.json
llms --chat request.json "Override prompt"
```

### Request Options

#### `--args PARAMS`

Add custom parameters to request (URL-encoded):

```bash
llms --args "temperature=0.7&seed=111" "Hello"
llms --args "max_completion_tokens=50" "Tell me a joke"
```

### Output Options

#### `--raw`

Display full JSON response:

```bash
llms --raw "What is 2+2?"
```

#### `--verbose`

Enable detailed logging:

```bash
llms --verbose "Hello"
llms --serve 8000 --verbose
```

#### `--logprefix PREFIX`

Custom log message prefix:

```bash
llms --verbose --logprefix "[DEBUG] " "Hello"
```

### Server Options

#### `--serve PORT`

Start HTTP server:

```bash
llms --serve 8000
llms --serve 3000 --verbose
```

#### `--root PATH`

Custom root directory for UI files:

```bash
llms --serve 8000 --root /path/to/ui
```

### Configuration Options

#### `--config FILE`

Use custom configuration file:

```bash
llms --config /path/to/config.json "Hello"
```

#### `--init`

Create default configuration:

```bash
llms --init
```

#### `--list, ls`

List providers and models:

```bash
llms --list
llms ls
llms ls groq anthropic
```

#### `--enable PROVIDER`

Enable one or more providers:

```bash
llms --enable groq
llms --enable openai anthropic grok
```

#### `--disable PROVIDER`

Disable one or more providers:

```bash
llms --disable ollama
llms --disable openai anthropic
```

#### `--default MODEL`

Set default model:

```bash
llms --default grok-4-fast
llms --default gemini-2.5-pro
```

#### `--check PROVIDER [MODELS...]`

Check provider status:

```bash
llms --check groq
llms --check groq kimi-k2 llama4:400b
```

### Help

#### `-h, --help`

Show help message:

```bash
llms --help
```

## Examples

### Text Generation

```bash
# Basic chat
llms "Explain quantum computing"

# With specific model
llms -m gemini-2.5-pro "Write a Python function to sort a list"

# With system prompt
llms -s "You are a quantum expert" "Explain entanglement"

# With custom parameters
llms --args "temperature=0.3&max_completion_tokens=100" "Tell me a joke"
```

### Image Analysis

```bash
# Default image template
llms --image ./screenshot.png

# With prompt
llms --image ./chart.png "Analyze this chart"

# With specific model
llms -m qwen2.5vl --image document.jpg "Extract text"

# Remote image
llms --image https://example.com/photo.jpg "Describe this"
```

### Audio Processing

```bash
# Default audio template (transcribe)
llms --audio recording.mp3

# With prompt
llms --audio meeting.wav "Summarize this meeting"

# With specific model
llms -m gpt-4o-audio-preview --audio interview.mp3 "Extract topics"
```

### Document Processing

```bash
# Default file template (summarize)
llms --file document.pdf

# With prompt
llms --file policy.pdf "Summarize key changes"

# With specific model
llms -m gpt-5 --file report.pdf "Extract action items"
```

### Custom Templates

```bash
# Use custom chat template
llms --chat custom-request.json "My prompt"

# Image with custom template
llms --chat image-request.json --image photo.jpg

# Audio with custom template
llms --chat audio-request.json --audio recording.mp3
```

### Server Mode

```bash
# Start server
llms --serve 8000

# With verbose logging
llms --serve 8000 --verbose

# Custom port
llms --serve 3000

# Custom UI root
llms --serve 8000 --root ./my-ui
```

### Configuration Management

```bash
# Initialize config
llms --init

# List all providers
llms ls

# List specific providers
llms ls groq anthropic openai

# Enable free providers
llms --enable openrouter_free google_free groq

# Enable paid providers
llms --enable openai anthropic grok

# Disable provider
llms --disable ollama

# Set default model
llms --default grok-4-fast

# Check provider status
llms --check groq
llms --check groq kimi-k2 llama4:400b gpt-oss:120b
```

## Environment Variables

### API Keys

```bash
OPENROUTER_API_KEY     # OpenRouter API key
GROQ_API_KEY           # Groq API key
GOOGLE_FREE_API_KEY    # Google Free tier API key
GOOGLE_API_KEY         # Google API key
ANTHROPIC_API_KEY      # Anthropic API key
OPENAI_API_KEY         # OpenAI API key
GROK_API_KEY           # Grok (X.AI) API key
DASHSCOPE_API_KEY      # Qwen API key
ZAI_API_KEY            # Z.ai API key
MISTRAL_API_KEY        # Mistral API key
CODESTRAL_API_KEY      # Codestral API key
```

### Other Settings

```bash
VERBOSE=1              # Enable verbose logging
```

## Next Steps

<Cards>
  <Card title="Configuration" href="/docs/configuration" />
  <Card title="Features" href="/docs/features" />
  <Card title="Multimodal Support" href="/docs/features/multimodal" />
</Cards>
