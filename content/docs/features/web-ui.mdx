---
title: Web UI
description: ChatGPT-like interface for all your LLMs
---

llms.py includes a modern, fast, and privacy-focused web interface that provides a ChatGPT-like experience for accessing all your local and remote LLMs.

## Overview

The UI is built with simplicity and privacy in mind:

- **No Build Tools**: Uses native ES modules
- **No npm Dependencies**: Pure JavaScript
- **Offline First**: Works entirely offline
- **Privacy Focused**: All data stored locally in IndexedDB
- **Fast & Lightweight**: Minimal footprint

## Key Features

### ğŸ¨ Modern Interface

Access all your LLMs through a clean, intuitive interface:

<Screenshot src="/img/llms-system-prompt.webp" />

### ğŸŒ™ Dark Mode

Built-in dark mode support that respects your system preference or can be toggled manually:

<Screenshot src="/img/llms-system-prompt-dark.webp" />

### ğŸ“ Rich Markdown & Syntax Highlighting

Full markdown rendering with syntax highlighting for popular programming languages:

<Screenshot src="/img/llms-syntax.webp" />

Code blocks include:
- Copy to clipboard on hover
- Language detection
- Line numbers
- Syntax highlighting

### ğŸ” Search History

Quickly find past conversations with built-in search:

<Screenshot src="/img/llms-search-python.webp" />

## Model Selector UI

With over 530 models from 23 providers now available, discovering and selecting the right model required a complete overhaul. 
The Model Selector has been completely redesigned as a full-featured dialog offering:

- **ğŸ” Smart Search & Discovery** - Instantly search across model names, IDs, and providers
- **ğŸ¯ Advanced Filtering** - Filter by name, providers & input and output modalities
- **ğŸ“Š Flexible Sorting** - Sort by Knowledge Cutoff, Release Date, Last Updated & Context
- **â­ Favorites System** - Star model card to add/remove to favorites quick list
- **ğŸ’ Rich Model Cards** - In depth model overview at a glance

<Screenshot src="/img/model-selector.webp" href="/docs/features/model-selector" />

### ğŸ“š System Prompt Library

Access 200+ professional system prompts for various use cases:

<Screenshot src="/img/llms-system-prompt.webp" />

Categories include:
- Technical assistance
- Creative writing
- Data analysis
- Code review
- Translation
- And many more...

See [System Prompts](/docs/features/system-prompts) docs for more details.

### âš™ï¸ Advanced Settings

Fine-tune AI requests with advanced options:

<Screenshot src="/img/llms-settings.webp" />

Available parameters:
- **Temperature** (0-2): Control response randomness
- **Max Completion Tokens**: Limit response length
- **Seed**: For deterministic outputs
- **Top P** (0-1): Nucleus sampling
- **Frequency Penalty** (-2.0 to 2.0): Reduce repetition
- **Presence Penalty** (-2.0 to 2.0): Encourage new topics
- **Stop Sequences**: Control where generation stops
- **Reasoning Effort**: For reasoning models
- **Top Logprobs** (0-20): Token probability analysis

### ğŸ­ Reasoning Support

Specialized rendering for reasoning models with thinking processes:

<Screenshot src="/img/llms-reasoning.webp" />

Shows:
- Thinking process (collapsed by default)
- Final response
- Clear separation between reasoning and output

### ğŸ“Š Token Metrics

See token usage for every message and conversation:

<Screenshot src="/img/llms-tokens-usage.webp" />

Displayed metrics:
- Per-message token count
- Thread total tokens
- Input vs output tokens
- Total cost
- Response time

### âœï¸ Edit & Redo

Edit previous messages or retry with different parameters:

- **Edit**: Modify user messages and rerun
- **Redo**: Regenerate AI responses
- Hover over messages to see options

### ğŸ’¾ Export/Import

Backup and transfer your chat history:

- **Export**: Save all conversations to JSON
- **Import**: Restore from backup
- Hold `ALT` while clicking Export to include analytics
- Transfer between browsers or instances

### ğŸ”Œ Enable/Disable Providers

Manage which providers are active in real-time:

<Screenshot src="/img/model-selector-providers.webp" />

- Toggle providers on/off
- Providers are tried in order
- Automatic failover if one fails
- See which models are available per provider

## Configuration

### UI Configuration

Customize the UI via `~/.llms/ui.json`:

```json
{
  "prompts": [...],
  "defaultModel": "grok-4-fast",
  "theme": "auto"
}
```

### Server Configuration

Start the server with custom options:

```bash
# Basic server
llms --serve 8000

# With verbose logging
llms --serve 8000 --verbose

# Custom root directory for UI files
llms --serve 8000 --root /path/to/ui
```

## Storage

All data is stored in browser IndexedDB:

- **Conversations**: All chat threads and messages
- **Activity Logs**: Request history for analytics
- **Settings**: UI preferences
- **User Profile**: If using OAuth

Data is tied to the browser origin (`http://localhost:PORT`), so running on different ports creates separate databases.

## Privacy

The UI is designed with privacy as a priority:

- âœ… No external requests (except to configured LLM providers)
- âœ… No tracking or analytics
- âœ… No ads
- âœ… No sign-ups required
- âœ… All data stored locally
- âœ… Works entirely offline
- âœ… Open source

## Performance

- Fast initial load
- Instant search
- Smooth scrolling
- Efficient rendering
- Minimal memory footprint

## Browser Support

Works with modern browsers supporting:
- ES Modules
- IndexedDB
- Fetch API
- CSS Grid/Flexbox

Tested on:
- Chrome/Edge (recommended)
- Firefox
- Safari

## Next Steps

<Cards>
  <Card title="Multimodal Support" href="/docs/features/multimodal" />
  <Card title="Analytics" href="/docs/features/analytics" />
  <Card title="Configuration" href="/docs/configuration" />
</Cards>
